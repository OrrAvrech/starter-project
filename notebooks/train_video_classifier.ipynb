{"cells":[{"cell_type":"markdown","metadata":{"id":"czSlAWs84AYm"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClPE8BQ47xet"},"outputs":[],"source":["!pip install -r \"/content/drive/MyDrive/mia_starter_project/requirements.txt\"\n","!apt install imagemagick -q"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EoOBXru89PrD","outputId":"792a7f2d-dcdb-4d85-cd72-fd14fe9183a1"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-fe20a3090d02>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import random\n","import torch\n","import imageio\n","import wandb\n","from pathlib import Path\n","from torch.utils.data import Dataset, DataLoader\n","from moviepy.editor import VideoFileClip, ImageSequenceClip\n","from IPython.display import Image\n",",\n","from torchvision.transforms import (\n","    Compose,\n","    Lambda,\n","    RandomCrop,\n","    RandomHorizontalFlip,\n","    Resize\n",")\n","from torchvision.transforms._transforms_video import CenterCropVideo\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    Normalize,\n","    ShortSideScale\n",")\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","from torchmetrics import Accuracy\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0F5dBCll66tb","outputId":"4ee2fd69-e498-4a39-8771-638d5d233cf5"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "]}],"source":["!wandb login"]},{"cell_type":"markdown","metadata":{"id":"Wyek_3F9RL8s"},"source":["## Data\n","The dataset is divided into 10 folders for each exercise.\n","`ActioClassificationDataset` basically maps folders into classes, and loads video chunks, according to `clip_duration = num_frames * sample_rate / vid.fps`, where `num_frames` and `sample_rate` are often determined by the parameters of the pre-trained model we are using."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iJE66CZlAR0j"},"outputs":[],"source":["project_dir = Path(\"/content/drive/MyDrive/mia_starter_project\")\n","data_dir = project_dir / \"dataset\"\n","train_dir = data_dir / \"train\"\n","val_dir = data_dir / \"validation_clean\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OmbdtNPK9bC-"},"outputs":[],"source":["class ActionClassificationDataset(Dataset):\n","    def __init__(\n","        self,\n","        data_dir: Path,\n","        num_frames: int,\n","        sample_rate: int,\n","        random_sampler: bool,\n","        transform=None,\n","    ):\n","        self.data_dir = data_dir\n","        self.labels = sorted([sub.name for sub in data_dir.iterdir() if sub.is_dir()])\n","        self.num_classes = len(self.labels)\n","        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.labels)}\n","        self.idx_to_class = {idx: cls for idx, cls in enumerate(self.labels)}\n","        self.clips = []\n","        self.num_frames = num_frames\n","        self.sample_rate = sample_rate\n","        self.random_sampler = random_sampler\n","        self.transform = transform\n","\n","        for cls in self.labels:\n","            class_dir = data_dir / cls\n","            for video_path in class_dir.glob(\"video/*.mp4\"):\n","                self.clips.append((video_path, self.class_to_idx[cls]))\n","\n","    def __len__(self) -> int:\n","        return len(self.clips)\n","\n","    def __getitem__(self, idx) -> tuple[torch.float32, int]:\n","        video_path, label = self.clips[idx]\n","\n","        with VideoFileClip(str(video_path)) as vid:\n","          clip_duration = self.num_frames * self.sample_rate / vid.fps\n","          subsample_fps = vid.fps / self.sample_rate\n","\n","          # by default take the start\n","          start_time = 0\n","          # for training, random sample\n","          if self.random_sampler:\n","              start_time = random.uniform(0, vid.duration - clip_duration)\n","          end_time = start_time + clip_duration\n","\n","          video_frames = list(vid.subclip(start_time, end_time).iter_frames(fps=subsample_fps))[\n","              : self.num_frames\n","          ]\n","\n","        video_np = np.moveaxis(np.array(video_frames), [0, -1], [1, 0])\n","        video_data = {\"video\": torch.from_numpy(video_np), \"audio\": None}\n","        video_data = self.transform(video_data)\n","        inputs = video_data[\"video\"]\n","        return inputs, label\n"]},{"cell_type":"markdown","metadata":{"id":"ARbTv5aPT7o0"},"source":["### X3D Params"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tn11Y9l8T63K"},"outputs":[],"source":["# X3D Params\n","model_name = \"x3d_xs\"\n","mean = [0.45, 0.45, 0.45]\n","std = [0.225, 0.225, 0.225]\n","model_transform_params = {\n","    \"x3d_xs\": {\n","        \"side_size\": 182,\n","        \"crop_size\": 182,\n","        \"num_frames\": 4,\n","        \"sampling_rate\": 12,\n","    },\n","    \"x3d_s\": {\n","        \"side_size\": 182,\n","        \"crop_size\": 182,\n","        \"num_frames\": 13,\n","        \"sampling_rate\": 6,\n","    },\n","    \"x3d_m\": {\n","        \"side_size\": 256,\n","        \"crop_size\": 256,\n","        \"num_frames\": 16,\n","        \"sampling_rate\": 5,\n","    }\n","}\n","\n","# Data/Training Params\n","flip_p = 0.5\n","batch_size = 16\n","epochs = 15\n","lr = 1e-3"]},{"cell_type":"markdown","metadata":{"id":"Tqbj3ACTUKWV"},"source":["Build datasets. Standard transformation are being used. More augmentations for training can be explored later on."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QkgpRdRt9kYu"},"outputs":[],"source":["config = model_transform_params[model_name]\n","config.update({\"flip_p\": flip_p,\n","                \"batch_size\": batch_size,\n","                \"epochs\": epochs,\n","                \"lr\": lr})\n","\n","train_transform = Compose(\n","    [\n","        ApplyTransformToKey(\n","            key=\"video\",\n","            transform=Compose(\n","                [\n","                    Lambda(lambda x: x / 255.0),\n","                    Normalize(mean, std),\n","                    ShortSideScale(size=config[\"side_size\"]),\n","                    CenterCropVideo(crop_size=(config[\"crop_size\"], config[\"crop_size\"])),\n","                    RandomHorizontalFlip(p=flip_p),\n","                ]\n","            ),\n","        ),\n","    ]\n",")\n","\n","val_transform = Compose(\n","    [\n","        ApplyTransformToKey(\n","            key=\"video\",\n","            transform=Compose(\n","                [\n","                    Lambda(lambda x: x / 255.0),\n","                    Normalize(mean, std),\n","                    ShortSideScale(size=config[\"side_size\"]),\n","                    CenterCropVideo(crop_size=(config[\"crop_size\"], config[\"crop_size\"]))\n","                ]\n","            ),\n","        ),\n","    ]\n",")\n","\n","train_ds = ActionClassificationDataset(data_dir=train_dir,\n","                                       num_frames=config[\"num_frames\"],\n","                                       sample_rate=config[\"sampling_rate\"],\n","                                       random_sampler=True,\n","                                       transform=train_transform)\n","\n","val_ds = ActionClassificationDataset(data_dir=val_dir,\n","                                    num_frames=config[\"num_frames\"],\n","                                    sample_rate=config[\"sampling_rate\"],\n","                                    random_sampler=False,\n","                                    transform=val_transform)\n","num_classes = train_ds.num_classes\n","print(f\"num classes: {num_classes}\")\n","print(f\"number of training samples: {len(train_ds)}\")\n","print(f\"number of validation samples: {len(val_ds)}\")"]},{"cell_type":"markdown","metadata":{"id":"QePEw43TTa8W"},"source":["By using an inverse normilzation class, we can easily visualize the videos for training after all transformations, except scaling and normalization."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BjdmfmIRAjil"},"outputs":[],"source":["class InverseNormalize(Normalize):\n","    def __init__(self, mean, std):\n","        mean = torch.as_tensor(mean)\n","        std = torch.as_tensor(std)\n","        std_inv = 1 / (std + torch.finfo(torch.float32).eps)\n","        mean_inv = -mean * std_inv\n","        super().__init__(mean=mean_inv, std=std_inv)\n","\n","    def __call__(self, tensor):\n","        return super().__call__(tensor.clone())\n","\n","\n","def inv_transform(sample):\n","  inv_transform = Compose([InverseNormalize(mean, std),\n","                           Lambda(lambda x: torch.clip((x * 255.0).to(torch.uint8), min=0, max=255)),\n","                           Lambda(lambda x: x.permute(1, 2, 3, 0))])\n","  sample_np = inv_transform(sample).detach().cpu().numpy()\n","  return sample_np"]},{"cell_type":"markdown","metadata":{"id":"XKMjqOUeUkXe"},"source":[" Visualize training sample. Notice that since `random_sampler=True` for the training dataset, each sample will randomly start from some time on each run."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CqCQJNA9SNQp"},"outputs":[],"source":["sample = train_ds[0]\n","sample_video = sample[0]\n","label = sample[1]\n","print(label)\n","print(val_ds.class_to_idx)\n","print(train_ds.class_to_idx)\n","\n","sample_video = inv_transform(sample_video)\n","clip = ImageSequenceClip(list(sample_video), fps=5)\n","clip.ipython_display()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7cz_s6ZqJTmM"},"outputs":[],"source":["train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"O9cgZU4oU65y"},"source":["## Modeling\n","X3D transfer learning model, where a Kinetics400 pre-trained model is load from Pytorch Hub. Then, the last fully connected is removed and replaced by a randomly initialized fully connected layer with 10 neurons for 10 classes."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iEe18AOGK_gk"},"outputs":[],"source":["class LitX3DTransfer(pl.LightningModule):\n","    def __init__(self, model_name: str, num_classes: int):\n","        super().__init__()\n","        self.model_name = model_name\n","        model = torch.hub.load(\"facebookresearch/pytorchvideo\", model_name, pretrained=True)\n","        layers = list(model.blocks.children())\n","        # feature extractor\n","        backbone = layers[:-1]\n","        self.feature_extractor = nn.Sequential(*backbone)\n","        # classifier\n","        self.fc = layers[-1]\n","        num_filters = self.fc.proj.in_features\n","        self.num_classes = num_classes\n","        self.fc.proj = nn.Linear(in_features=num_filters, out_features=num_classes, bias=True)\n","        # step outputs\n","        self.training_step_outputs = []\n","        self.validation_step_outputs = []\n","\n","    def forward(self, x):\n","        representations = self.feature_extractor(x)\n","        predictions = self.fc(representations)\n","        return predictions\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        self.feature_extractor.eval()\n","        with torch.no_grad():\n","          x = self.feature_extractor(x)\n","        y_hat = self.fc(x)\n","        loss = F.cross_entropy(y_hat, y)\n","        acc = self.accuracy(y_hat, y)\n","\n","        artifacts = {\"loss\": loss,\n","                     \"acc\": acc}\n","\n","        self.log(\"train_acc\", acc, prog_bar=True, on_epoch=True)\n","        self.log(\"train_loss\", loss.item(), prog_bar=True, on_epoch=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        loss = F.cross_entropy(y_hat, y)\n","        acc = self.accuracy(y_hat, y)\n","\n","        artifacts = {\"loss\": loss,\n","                     \"acc\": acc}\n","\n","        self.log(\"val_acc\", acc, prog_bar=True)\n","        self.log(\"val_loss\", loss.item(), prog_bar=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=lr)\n","\n","    @staticmethod\n","    def accuracy(predictions, labels):\n","        classes = torch.argmax(predictions, dim=1)\n","        mean_acc = torch.mean((classes == labels).float())\n","        return mean_acc"]},{"cell_type":"markdown","metadata":{"id":"ugDkRKBgVDER"},"source":["## Training\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"39hODz7VzSd_"},"outputs":[],"source":["x3d = LitX3DTransfer(model_name=model_name, num_classes=num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"x7fKYHt2zr-X"},"outputs":[],"source":["early_stop_cb = pl.callbacks.EarlyStopping(monitor=\"val_loss\")\n","model_ckpt_cb = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n","wandb_logger = pl.loggers.WandbLogger(project=\"mia-starter-project\")\n","\n","# logs_dir = Path(\"/content/drive/MyDrive/mia_starter_project/logs\") / run.name\n","trainer = pl.Trainer(max_epochs=epochs, callbacks=[early_stop_cb, model_ckpt_cb], logger=wandb_logger)\n","trainer.fit(model=x3d, train_dataloaders=train_loader, val_dataloaders=val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dciY-RnQ55R4"},"outputs":[],"source":["wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"Lwj-5GvegcMs"},"source":["## Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DIE2dzXugf2R"},"outputs":[],"source":["ckpt = torch.load(\"/content/drive/MyDrive/mia_starter_project/logs/smooth-voice-6/lightning_logs/version_0/checkpoints/epoch=4-step=75.ckpt\", map_location=torch.device(\"cpu\"))\n","infer_model = LitX3DTransfer(model_name, num_classes)\n","infer_model.load_state_dict(ckpt[\"state_dict\"])\n","infer_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-KV7Wy04JYvU"},"outputs":[],"source":["!pip install --upgrade imageio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AX2uczDH9h-W"},"outputs":[],"source":["from IPython.display import HTML, display\n","import imageio\n","from PIL import Image\n","\n","\n","def show_batch_vid(batch_vid):\n","  vid = inv_transform(x)\n","  # im = Image.from_array(vid)\n","  # clip = ImageSequenceClip(list(vid), fps=5)\n","  # clip.write_videofile(\"check.mp4\", logger=None)\n","  # clip = VideoFileClip(\"check.mp4\").preview()\n","  vidd = [Image.fromarray(x).convert(\"RGB\") for x in list(vid)]\n","  imageio.mimwrite(\"check.gif\", vidd, duration=0.1)\n","\n","\n","\n","def grid(video_filenames):\n","  # Create an HTML table to display the videos in a grid\n","  video_grid_html = \"<table style='width:100%'>\"\n","\n","  # Set the number of columns in the grid (e.g., 2 columns)\n","  num_columns = 2\n","\n","  for i, video_filename in enumerate(video_filenames):\n","      if i % num_columns == 0:\n","          video_grid_html += \"<tr>\"\n","\n","      video_grid_html += f\"<td><video width='182' height='182' controls><source src='{video_filename}' type='video/mp4'></video></td>\"\n","\n","      if (i + 1) % num_columns == 0 or (i + 1) == len(video_filenames):\n","          video_grid_html += \"</tr>\"\n","\n","  video_grid_html += \"</table>\"\n","\n","  # Display the video grid in Colab\n","  display(HTML(video_grid_html))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QC1HsWig4KHu"},"outputs":[],"source":["# predictions = []\n","# for x, y in val_ds:\n","#   show_batch_vid(x)\n","#   # y_hat = infer_model(x)\n","#   # pred = torch.argmax(y_hat, dim=1)\n","#   break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BKbgnSDMGEWI"},"outputs":[],"source":["grid([\"/content/check.mp4\"])"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1fII7W5zYVneJM4RtNEeN-qjIjONhqzMj","authorship_tag":"ABX9TyMiESuXelyPPW3Cnt1C6mQX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}